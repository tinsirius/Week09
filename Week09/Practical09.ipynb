{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinsirius/Week09/blob/colab/Week09/Practical09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ECE4078 2023 Workshop 9: Human Robot Interaction (HRI)\n",
        "\n",
        "By Dr. Leimin Tian\n",
        "\n",
        "In this notebook, you will use hand gestures to communicate to a robot: üëç, üëé, ‚úåÔ∏è, ‚òùÔ∏è, ‚úä, üëã, ü§ü\n",
        "\n",
        "(\"Thumb_Up\", \"Thumb_Down\", \"Victory\", \"Pointing_Up\", \"Closed_Fist\", \"Open_Palm\", \"ILoveYou\")"
      ],
      "metadata": {
        "id": "oFUxIJQhwBO4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with installing MediaPipe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "### The hand gesture recognizer is developed with MediaPipe Python API  (Copyright 2023 The MediaPipe Authors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title MediaPipe Python API Licensed under the Apache License, Version 2.0\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxbHBsF-8Y_l"
      },
      "outputs": [],
      "source": [
        "!pip install -q mediapipe==0.10.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "Download an off-the-shelf model. See [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer#models) for more details about the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W_6sv5-JUdY"
      },
      "source": [
        "### Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H4aPO-hvbw3r"
      },
      "outputs": [],
      "source": [
        "#@markdown Functions to visualize the gesture recognition results. <br/> Run the following cell to activate the functions.\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapipe as mp\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import math\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.left': False,\n",
        "    'axes.spines.bottom': False,\n",
        "    'xtick.labelbottom': False,\n",
        "    'xtick.bottom': False,\n",
        "    'ytick.labelleft': False,\n",
        "    'ytick.left': False,\n",
        "    'xtick.labeltop': False,\n",
        "    'xtick.top': False,\n",
        "    'ytick.labelright': False,\n",
        "    'ytick.right': False\n",
        "})\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "\n",
        "def display_one_image(image, title, subplot, titlesize=16):\n",
        "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
        "    plt.subplot(*subplot)\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "\n",
        "\n",
        "def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):\n",
        "    \"\"\"Displays a batch of images with the gesture category and its score along with the hand landmarks.\"\"\"\n",
        "    # Images and labels.\n",
        "    images = [image.numpy_view() for image in images]\n",
        "    gestures = [top_gesture for (top_gesture, _) in results]\n",
        "    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]\n",
        "\n",
        "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images) // rows\n",
        "\n",
        "    # Size and spacing.\n",
        "    FIGSIZE = 10.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols, 1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "\n",
        "    # Display gestures and hand landmarks.\n",
        "    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):\n",
        "        title = f\"{gestures.category_name} ({gestures.score:.2f})\"\n",
        "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
        "        annotated_image = image.copy()\n",
        "\n",
        "        for hand_landmarks in multi_hand_landmarks_list[i]:\n",
        "          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "          hand_landmarks_proto.landmark.extend([\n",
        "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
        "          ])\n",
        "\n",
        "          mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks_proto,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "\n",
        "        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)\n",
        "\n",
        "    # Layout.\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the hand gesture recognizer"
      ],
      "metadata": {
        "id": "rvNobwff0Qon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# create an GestureRecognizer object\n",
        "base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
        "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
        "recognizer = vision.GestureRecognizer.create_from_options(options)"
      ],
      "metadata": {
        "id": "Jo6rnN9h0G0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## 1. Test the hand gesture recognizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose which option to use for getting the test image"
      ],
      "metadata": {
        "id": "5-Z0a4vsvcXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the input source\n",
        "img_option = 1 # Option 1: use your webcam\n",
        "# img_option = 2 # Option 2: upload image from your local machine"
      ],
      "metadata": {
        "id": "Eou3YEtmvGPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Option 1: use your webcam"
      ],
      "metadata": {
        "id": "JWNdmRa8t98L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown (webcam input utility functions based on [this git repo](https://github.com/tugstugi/dl-colab-notebooks))\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "def take_photo_robot(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Give Instruction (press after posing)';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "1-jwUDq2tv5w",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "frame_ID = 0\n",
        "if img_option == 1:\n",
        "    try:\n",
        "        frame_name='frame_' + str(frame_ID) + '.jpg'\n",
        "        filename = take_photo(frame_name, 0.8)\n",
        "        print('Photo saved as {} (see left side menu for all files)'.format(filename))\n",
        "\n",
        "        # Show the image which was just taken.\n",
        "        display(Image(filename))\n",
        "        IMAGE_FILENAMES = [filename]\n",
        "    except Exception as err:\n",
        "        # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "        # grant the page permission to access it.\n",
        "        print(str(err))"
      ],
      "metadata": {
        "id": "uTMgQLwTuI37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvwwAdKlgpSo"
      },
      "source": [
        "#### Option 2: upload from you local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cW_V2HvguvE"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "if img_option == 2:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded:\n",
        "        content = uploaded[filename]\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(content)\n",
        "    IMAGE_FILENAMES = list(uploaded.keys())\n",
        "\n",
        "    print('Uploaded files:', IMAGE_FILENAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "### Running inference and visualizing the results\n",
        "\n",
        "See [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer/python) for more info.\n",
        "\n",
        "*Note: Gesture Recognizer also returns the hand landmark it detects from the image, together with other useful information such as whether the hand(s) detected are left hand or right hand.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHqaswD6M8iO"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "results = []\n",
        "for image_file_name in IMAGE_FILENAMES:\n",
        "  # load the input image\n",
        "  image = mp.Image.create_from_file(image_file_name)\n",
        "\n",
        "  # recognize gestures in the input image\n",
        "  recognition_result = recognizer.recognize(image)\n",
        "\n",
        "  # process and visualize the recognition result\n",
        "  images.append(image)\n",
        "  top_gesture = recognition_result.gestures[0][0]\n",
        "  hand_landmarks = recognition_result.hand_landmarks\n",
        "  results.append((top_gesture, hand_landmarks))\n",
        "\n",
        "display_batch_of_images_with_gestures_and_hand_landmarks(images, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Using gestures to give feedback to a robot"
      ],
      "metadata": {
        "id": "ywo0pnGcQxOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define your hand gesture commands\n",
        "Here the robot gets a reward of +1.0 if it sees a thumb up, -1.0 for thumb down, 0 for everything else."
      ],
      "metadata": {
        "id": "BM3hIxLknL7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JVO3rvPD4RN"
      },
      "outputs": [],
      "source": [
        "# command dictionary for matching recognised hand gesture to robot actions\n",
        "# available labels: ['Thumb_Up','Thumb_Down','Victory','Pointing_Up','Closed_Fist','Open_Palm','ILoveYou','None']\n",
        "def commands(top_gesture, conf_threshold=0.5):\n",
        "    label = top_gesture.category_name\n",
        "    conf = top_gesture.score\n",
        "    reward = 0\n",
        "    if conf >= conf_threshold:\n",
        "        if label == 'Thumb_Up':\n",
        "            print(f'Human instruction is {label}: Good Job!')\n",
        "            reward = 1\n",
        "        elif label == 'Thumb_Down':\n",
        "            print(f'Human instruction is {label}: Try harder.')\n",
        "            reward = -1\n",
        "        else:\n",
        "            print(f'Unknown instruction: hand gesture recognised as {label}')\n",
        "            reward = 0\n",
        "    else:\n",
        "        print(f'Human gesture may be {label}, but unsure as my confidence is only {conf:.2f}')\n",
        "        reward = 0\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The interaction scenario\n",
        "The simple scenario we are creating here is a robot and a human crossing paths when walking to their own goal.\n",
        "\n",
        "We generate a number of episodes by varying the speed that the robot moves"
      ],
      "metadata": {
        "id": "M14T7Z9iMDZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate random episodes where a robot and a person cross path at different speed\n",
        "import numpy as np\n",
        "np.random.seed(0) # seed for reproducibility\n",
        "\n",
        "# how many episodes to generate\n",
        "ep_num = 5\n",
        "# a default list of speeds to use\n",
        "speeds = np.array([1,3,5,7,10])\n",
        "\n",
        "# you can also generate the speeds randomly\n",
        "#speeds = np.random.randint(1, 11, ep_num)\n",
        "speeds = np.sort(speeds)\n",
        "\n",
        "print(f'Speed in generated episodes: {speeds}')"
      ],
      "metadata": {
        "id": "v72YXy4F9UtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the scenario\n",
        "# part of this cell was implemented with the help of ChatGPT\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "import warnings\n",
        "import matplotlib.cbook\n",
        "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
        "\n",
        "# define the start and target points for the person (dot) and robot (square)\n",
        "# feel free to change the map and the start and goal of human/robot\n",
        "dot_start = (0, 0)\n",
        "dot_target = (10, 9)\n",
        "square_start = (0, 10)\n",
        "square_target = (9, 0)\n",
        "\n",
        "# create map with background grid\n",
        "fig, ax = plt.subplots()\n",
        "plt.xlim(-1, 11)\n",
        "plt.ylim(-1, 11)\n",
        "ax.set_xlabel('red dot is a person, blue square is a robot')\n",
        "ax.set_ylabel('a simple near miss map')\n",
        "ax.grid(True)\n",
        "\n",
        "# paint the start and finish location\n",
        "plt.plot(0,0,'>', color = 'red', markersize=15)\n",
        "plt.plot(10,9,'*', color = 'red', markersize=15)\n",
        "plt.plot(0,10,'>', color = 'blue', markersize=15)\n",
        "plt.plot(9,0,'*', color = 'blue', markersize=15)\n",
        "\n",
        "# create a dot that will move from dot_start to dot_target\n",
        "dot, = ax.plot([], [], 'ro', markersize=10)\n",
        "\n",
        "# create a square that will move from square_start to square_target\n",
        "square, = ax.plot([], [], 'bs', markersize=10)\n",
        "\n",
        "# initialize the animation\n",
        "def init(robot_speed):\n",
        "    dot.set_data([], [])\n",
        "    square.set_data([], [])\n",
        "    return dot, square, robot_speed\n",
        "\n",
        "# update the plot per frame\n",
        "def update(frame, robot_speed):\n",
        "    dot_progress = frame / num_frames\n",
        "    x_dot = dot_start[0] + dot_progress * (dot_target[0] - dot_start[0])\n",
        "    y_dot = dot_start[1] + dot_progress * (dot_target[1] - dot_start[1])\n",
        "    dot.set_data(x_dot, y_dot)\n",
        "\n",
        "    # the robot's speed is robot_speed times of the human's speed\n",
        "    square_progress = (frame  / num_frames) * robot_speed\n",
        "    x_square = square_start[0] + square_progress * (square_target[0] - square_start[0])\n",
        "    y_square = square_start[1] + square_progress * (square_target[1] - square_start[1])\n",
        "    square.set_data(x_square, y_square)\n",
        "\n",
        "    return dot, square"
      ],
      "metadata": {
        "id": "dPfJJk2ETz9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view an example episode\n",
        "num_frames = 100 # Total number of frames\n",
        "interval = 50 # Interval in milliseconds between frames\n",
        "\n",
        "# show slowest robot setting\n",
        "ID = 0\n",
        "robot_speed = speeds[ID]\n",
        "\n",
        "# create the animation\n",
        "animation = FuncAnimation(fig, update, frames=num_frames, fargs=(robot_speed,), init_func=lambda: init(robot_speed), blit=False, interval=interval)\n",
        "\n",
        "# display the animation\n",
        "HTML(animation.to_html5_video())"
      ],
      "metadata": {
        "id": "_WSpbhDkzBkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and save a few example episodes\n",
        "num_frames = 100 # Total number of frames\n",
        "interval = 50 # Interval in milliseconds between frames controlled by speed\n",
        "ID = 0\n",
        "\n",
        "while ID < ep_num:\n",
        "    robot_speed = speeds[ID]\n",
        "\n",
        "    # Create the animation\n",
        "    animation = FuncAnimation(fig, update, frames=num_frames, fargs=(robot_speed,), init_func=lambda: init(robot_speed), blit=False, interval=interval)\n",
        "    video_name = 'interaction_' + str(ID) + '.mp4'\n",
        "    animation.save(video_name, writer=\"ffmpeg\")\n",
        "\n",
        "    ID = ID + 1"
      ],
      "metadata": {
        "id": "pSxYQeY1fd5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's give it a try\n",
        "- webcam required for interactive feedback\n",
        "- you can use a simulated human instead that provides a feedback list based on the speeds"
      ],
      "metadata": {
        "id": "fM_CbEOHRLaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change to False if you want to use simulated feedback instead of your webcam\n",
        "webcam = True"
      ],
      "metadata": {
        "id": "6ikXz9JlMfLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a simulated human\n",
        "# prefer speed between [5,6,7], don't like too slow [1,2] or too fast [9,10], otherwise [3,4,8] indifferent\n",
        "feedback_sim = -1 + 1 * (speeds>2) - 1 * (speeds>8) + 1 * (speeds>4) - 1 * (speeds>7)\n",
        "print(f'Human feedback given: {feedback_sim}')"
      ],
      "metadata": {
        "id": "PFdMYqSAMWM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# video display function\n",
        "from IPython.display import HTML, clear_output\n",
        "from base64 import b64encode\n",
        "\n",
        "def show_video(video_path, video_width = 500):\n",
        "\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "metadata": {
        "id": "8E-IeLVEztHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# give responses to each interaction episode\n",
        "from IPython.display import Video\n",
        "ID = 0\n",
        "reward = 0\n",
        "feedback = []\n",
        "\n",
        "# webcam interactive feedback\n",
        "if webcam:\n",
        "    while ID < ep_num:\n",
        "        # show example episode\n",
        "        ep_video = 'interaction_' + str(ID) + '.mp4'\n",
        "        print(f'Showing example episode {ep_video} with robot speed = {speeds[ID]}')\n",
        "        display(show_video(ep_video))\n",
        "        print('\\nPress \\'Give Instruction\\' button to capture you hand gestures as feedback to the robot')\n",
        "\n",
        "        # capture gesture response from webcam\n",
        "        frame_name = 'frame_' + str(ID) + '.jpg'\n",
        "        filename = take_photo_robot(frame_name, 0.8)\n",
        "        ID = ID + 1\n",
        "        image = mp.Image.create_from_file(filename)\n",
        "\n",
        "        # recognize gesture in input image as feedback\n",
        "        recognition_result = recognizer.recognize(image)\n",
        "        if recognition_result.gestures == []:\n",
        "            print('No hands found!')\n",
        "            reward = 0\n",
        "        else:\n",
        "            top_gesture = recognition_result.gestures[0][0]\n",
        "            reward = commands(top_gesture, 0.5)\n",
        "        print(f'\\nReward given by human = {reward}')\n",
        "        feedback.append(reward)\n",
        "\n",
        "        # remove old video before showing new one\n",
        "        clear_output()\n",
        "\n",
        "# use simulated feedback if webcam not used\n",
        "else:\n",
        "    # use simulated human\n",
        "    feedback = feedback_sim\n",
        "\n",
        "print(f'Human feedback given: {feedback}')"
      ],
      "metadata": {
        "id": "pSaTxt7vRH5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we design a simple robot policy:\n",
        "\n",
        "\n",
        "*   If there are episodes rated as preferred by the human, use the fastest speed that the human prefer\n",
        "*   If there are no episodes rated as preferred, but there are episodes rated as indifferent, use the fastest speed that the human is indifferent about\n",
        "*   If the humans rated all episodes as undesirable, show a new set of episodes with speed different from those already shown.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FqBU4Q1cCItf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print human feedback summary\n",
        "feedback = np.array(feedback)\n",
        "likes = speeds[feedback == 1]\n",
        "dislikes = speeds[feedback == -1]\n",
        "indifferent = speeds[feedback == 0]\n",
        "print(f'Preferred speeds: {likes}; Undesirable speeds: {dislikes}; Indifferent speeds: {indifferent}')"
      ],
      "metadata": {
        "id": "lKH61dPoUiKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decide on speed based on human feedback\n",
        "chosen_speed = 0\n",
        "\n",
        "# new speed options that's not yet shown to the user\n",
        "all_speeds = np.arange(1, 11, dtype=int)\n",
        "new_speeds = np.setdiff1d(all_speeds,speeds)\n",
        "new_speeds = np.sort(new_speeds)\n",
        "\n",
        "# if there are episodes that the user liked, use the fastest speed that's rated as liked\n",
        "if len(likes) > 0:\n",
        "    likes = np.sort(likes)\n",
        "    chosen_speed = likes[-1]\n",
        "    print(f'Thank you for the feedback! I will use speed = {chosen_speed} then, since you liked it and it will get me to my goal faster.')\n",
        "# if there are no episodes the user liked, use the fastest speed that they didn't hate\n",
        "elif len(indifferent) > 0:\n",
        "    indifferent = np.sort(indifferent)\n",
        "    chosen_speed = indifferent[-1]\n",
        "    print(f'Thank you for the feedback! I will use speed = {chosen_speed} then, since you didn\\'t hate it and it will get me to my goal faster.')\n",
        "# if they dislike all episodes, show some new options\n",
        "else:\n",
        "    print(f'Thank you for the feedback! I\\'m sorry that you didn\\'t like any of the shown speeds, here are some alternatives: {new_speeds}')"
      ],
      "metadata": {
        "id": "VP99Ko6ECGLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_feedback = []\n",
        "# show the chosen speed\n",
        "if chosen_speed > 0:\n",
        "    robot_speed = chosen_speed\n",
        "    # create the animation\n",
        "    animation = FuncAnimation(fig, update, frames=num_frames, fargs=(robot_speed,), init_func=lambda: init(robot_speed), blit=False, interval=interval)\n",
        "    video_name = 'interaction_chosen_speed_' + str(chosen_speed) + '.mp4'\n",
        "\n",
        "    # display the animation\n",
        "    print(f'Here is what we\\'ve decided: robot speed = {chosen_speed}')\n",
        "    animation.save(video_name, writer=\"ffmpeg\")\n",
        "    display(show_video(video_name))\n",
        "\n",
        "# show an alternative with the slowest new speed\n",
        "else:\n",
        "    robot_speed = new_speeds[0]\n",
        "    # create the animation\n",
        "    animation = FuncAnimation(fig, update, frames=num_frames, fargs=(robot_speed,), init_func=lambda: init(robot_speed), blit=False, interval=interval)\n",
        "    video_name = 'interaction_new_speed_' + str(robot_speed) + '.mp4'\n",
        "    animation.save(video_name, writer=\"ffmpeg\")\n",
        "\n",
        "    # display the animation\n",
        "    print(f'How about this one: robot speed = {robot_speed}')\n",
        "    display(show_video(video_name))\n",
        "\n",
        "    # capture gesture response from webcam\n",
        "    frame_name = 'frame_new_speed_' + str(robot_speed) + '.jpg'\n",
        "    filename = take_photo_robot(frame_name, 0.8)\n",
        "    image = mp.Image.create_from_file(filename)\n",
        "\n",
        "    # recognize gesture in input image as feedback\n",
        "    recognition_result = recognizer.recognize(image)\n",
        "    if recognition_result.gestures == []:\n",
        "        print('No hands found!')\n",
        "        reward = 0\n",
        "    else:\n",
        "        top_gesture = recognition_result.gestures[0][0]\n",
        "        reward = commands(top_gesture, 0.5)\n",
        "    print(f'\\nReward given by human = {reward}')\n",
        "    new_feedback.append(reward)"
      ],
      "metadata": {
        "id": "soxTjkoNFnJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to modify the gesture commands, map, robot's decision policy etc. and develop your own gesture controlled robot!\n",
        "\n",
        "Beyond gestures, there are many ways humans and robots can interact naturally."
      ],
      "metadata": {
        "id": "VcuixQ5Fn4Vf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8W_6sv5-JUdY"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
